from __future__ import annotations

import os
import platform
import sys
from contextlib import contextmanager
from dataclasses import dataclass
from json import dump, load
from pathlib import Path
from statistics import mean
from subprocess import run
from time import perf_counter
from typing import Any, Callable, Generator, Iterator, Literal

from rich import print
from rich.align import Align
from rich.progress import track
from rich.text import Text

EMOJIS = {
    "Linux": "ðŸ§",
    "Windows": "ðŸªŸ",
    "macos": "ðŸŽ",
}

CURRENT_PLATFORM = platform.system()
PLATFORM_EMOJI = EMOJIS[CURRENT_PLATFORM]
PYTHON_VERSION: tuple[int, int] = sys.version_info[:2]
NORMALIZED_PYTHON_VERSION = f"{PYTHON_VERSION[0]}.{PYTHON_VERSION[1]}"
BENCHMARK_DIRECTORY = Path(__file__).parent / "benchmarks"
TEST_BENCHMARK_DIRECTORY = Path(__file__).parent / "benchmarks_test"
def centered_text(text: str) -> Align:
    return Align.center(Text(text))


@dataclass
class Stats:
    name: str
    warmup: list[float]
    benchmark: list[float]

    @classmethod
    def from_dict(cls, stats_dict: dict[str, list[float]], name: str) -> Stats:
        return cls(
            name,
            stats_dict["warmup"],
            stats_dict["benchmark"],
        )


@dataclass
class Benchmark:
    name: str
    nuitka_version: str | None = None
    file_json: dict[str, dict[str, list[float]]] | None = None
    nuitka_stats: Stats | None = None
    cpython_stats: Stats | None = None
    factory: Benchmark | None = None
    python_version: tuple[int, int] | None = None

    def parse_stats(self, stats: dict[str, dict[str, list[float]]]) -> None:
        self.nuitka_stats = Stats.from_dict(stats["nuitka"], "nuitka")
        self.cpython_stats = Stats.from_dict(stats["cpython"], "cpython")

    @staticmethod
    def parse_file_name(file_name: str) -> tuple[str, str, tuple[int, int]]:
        platform, version, nuitka_version = file_name.split("-")
        return platform, nuitka_version, tuple(map(int, version.split(".")))

    @property
    def normalized_python_version(self) -> str:
        return f"{self.python_version[0]}.{self.python_version[1]}"

    @classmethod
    def from_path(cls, file_path: Path, benchmark_name: str) -> Benchmark:

        if not file_path.stat().st_size > 0:
            raise FileNotFoundError(f"File {file_path} does not exist or is empty")

        with open(file_path) as f:
            file_json = load(f)

        file_info = cls.parse_file_name(file_path.stem)

        benchmark = cls(
            name=benchmark_name,
            nuitka_version=file_info[1],
            python_version=file_info[2],
        )
        benchmark.parse_stats(file_json)

        return benchmark



    def to_json_file(self, file_path: Path) -> None:

        if not self.nuitka_stats or not self.cpython_stats:
            msg = "Stats not found"
            raise ValueError(msg)

        contents = {
            "nuitka": {
                "warmup": self.nuitka_stats.warmup,
                "benchmark": self.nuitka_stats.benchmark,
            },
            "cpython": {
                "warmup": self.cpython_stats.warmup,
                "benchmark": self.cpython_stats.benchmark,
            },
        }
        with open(file_path, "w") as f:
            dump(contents, f)

    def calculate_stats(self, which: Literal["nuitka", "cpython"]) -> float:

        if which.lower() not in ["nuitka", "cpython"]:
            msg = "Invalid value for 'which' parameter"
            raise ValueError(msg)

        if not self.nuitka_stats or not self.cpython_stats:
            msg = "Stats not found"
            raise ValueError(msg)

        stats = self.nuitka_stats if which.lower() == "nuitka" else self.cpython_stats

        is_warmup_skewed: bool = min(stats.warmup) == stats.warmup[0]
        is_benchmark_skewed: bool = min(stats.benchmark) == stats.benchmark[0]

        warmup = mean(stats.warmup[is_warmup_skewed:])
        benchmark = mean(stats.benchmark[is_benchmark_skewed:])

        return min(warmup, benchmark)


    def format_stats(self) -> Align:
        nuitka_stats = self.calculate_stats("nuitka")
        cpython_stats = self.calculate_stats("cpython")

        if nuitka_stats < cpython_stats:
            difference = (cpython_stats - nuitka_stats) / cpython_stats * 100
            return Align.center(Text(f"+{difference:.2f}%", style="green"))
        elif nuitka_stats > cpython_stats:
            difference = (nuitka_stats - cpython_stats) / cpython_stats * 100
            return Align.center(Text(f"-{difference:.2f}%", style="red"))
        else:
            difference = (nuitka_stats - cpython_stats) / cpython_stats * 100
            return Align.center(Text(f"{difference:.2f}%", style="yellow"))


class Timer:
    def __init__(self) -> None:
        self.start: float = 0
        self.end: float = 0

        self.time_taken: float = 0

    def __enter__(self) -> Timer:
        self.start = perf_counter()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb) -> None:  # type: ignore
        self.end = perf_counter()

        self.time_taken = self.end - self.start

    def __call__(self, func: Callable[..., Any]) -> Callable[..., Any]:
        def wrapper(*args: Any, **kwargs: Any) -> Any:
            with self:
                return func(*args, **kwargs)

        return wrapper


@contextmanager
def temporary_directory_change(path: Path) -> Iterator[None]:
    if not path.exists():
        msg = f"Directory {path} does not exist"
        raise FileNotFoundError(msg)
    current_directory = Path.cwd()
    os.chdir(path)
    yield
    os.chdir(current_directory)


def run_benchmark(
    benchmark: Path,
    python_executable: Path,
    iterations: int,
    type: str,
    nuitka_version: str,
    count: int,
    number_of_benchmarks: int,
    compilation_type: Literal["onefile", "standalone", "accelerated"] = "accelerated",
) -> dict[str, list[float]]:
    local_results: dict[str, list[float]] = {
        "warmup": [],
        "benchmark": [],
    }

    comp_types = {
        "accelerated": {
            "Linux": "run_benchmark.dist/run_benchmark.bin",
            "Windows": "run_benchmark.dist\\run_benchmark.cmd",
        },
        "standalone": {
            "Linux": "run_benchmark.dist/run_benchmark",
            "Windows": "run_benchmark.dist\\run_benchmark.cmd",
        },
        "onefile": {
            "Linux": "run_benchmark.dist/run_benchmark",
            "Windows": "run_benchmark.dist\\run_benchmark.cmd",
        },
    }
    run_command = {
        "Nuitka": [
            comp_types[compilation_type][CURRENT_PLATFORM],
        ],
        "CPython": [python_executable, "run_benchmark.py"],
    }

    description_dict = {
        "CPython": f"{benchmark.name} with {type} {NORMALIZED_PYTHON_VERSION}",
        "Nuitka": f"{benchmark.name} with Nuitka {nuitka_version} | CPython: {NORMALIZED_PYTHON_VERSION}",
    }

    for _ in track(
        range(iterations),
        description=f"[{count}/{number_of_benchmarks}] Warming up {description_dict[type]}",
        total=iterations,
    ):
        with Timer() as timer:
            res = run(run_command[type], check=False)  # type: ignore
            if res.returncode != 0:
                msg = f"Failed to run benchmark {benchmark.name} due to {res.stderr!r}"
                raise RuntimeError(
                    msg
                )

        local_results["warmup"].append(timer.time_taken)

    for _ in track(
        range(iterations),
        description=f"[{count}/{number_of_benchmarks}] Benchmarking {description_dict[type]}",
        total=iterations,
    ):
        with Timer() as timer:
            res = run(run_command[type], check=False)  # type: ignore
            if res.returncode != 0:
                msg = f"Failed to run benchmark {benchmark.name}"
                raise RuntimeError(msg)

        local_results["benchmark"].append(timer.time_taken)

    print(f"Completed benchmarking {benchmark.name} with {type}")

    return local_results


def is_in_venv() -> bool:
    # https://stackoverflow.com/a/1883251
    return sys.prefix != sys.base_prefix


def _get_benchmarks(test: bool = False) -> Iterator[Path]:
    bench_dir = TEST_BENCHMARK_DIRECTORY if test else BENCHMARK_DIRECTORY
    for benchmark_case in bench_dir.iterdir():
        if not benchmark_case.is_dir() or not benchmark_case.name.startswith("bm_"):
            continue
        yield benchmark_case


def get_visualizer_setup(
    test: bool = False,
) -> Generator[tuple[str, list[Benchmark]], None, None]:
    for benchmark in _get_benchmarks(test=test):
        results_dir = benchmark / "results"
        if not results_dir.exists():
            print(
                f"Skipping benchmark {benchmark.name}, because {results_dir} does not exist"
            )
            continue

        benchmark_case_group: list[Benchmark] = []

        for result in results_dir.iterdir():
            if result.is_file():
                try:
                    benchmark = Benchmark.from_path(result, benchmark.name)
                except FileNotFoundError as e:
                    print(e)
                    continue
                benchmark_case_group.append(benchmark)
        benchmark_case_group.sort(key=lambda x: x.python_version, reverse=True)

        yield benchmark.name, benchmark_case_group



def get_benchmark_setup() -> list[Path]:
    return list(_get_benchmarks())


def setup_benchmark_enviroment(
    requirements_path: Path,
    silent: bool = False,
) -> Path:
    python_executable = Path(sys.executable)

    try:

        # cmd = "--pgo --lto=yes"
        cmd = "--lto=yes"  # let's not use PGO for now

        commands = [
            f"{python_executable} -m nuitka --output-dir=run_benchmark.dist {cmd} run_benchmark.py".split()
        ]
        if requirements_path.exists():
            commands.insert(
                0,
                f"{python_executable} -m pip install -r {requirements_path.absolute()}".split(),
            )
        for command in commands:
            res = run(command, check=False)
            if res.returncode != 0:
                print(f"Failed to run command {command}")

    except Exception as e:
        msg = f"Failed to setup benchmark enviroment\n{e}"
        raise RuntimeError(msg)

    return python_executable
